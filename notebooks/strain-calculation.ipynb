{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how different types of Gravitational Wave Background (GWB) strain amplitude calculations are performed.\n",
    "\n",
    "Consult the following references:\n",
    "* [Phinney-2001](https://ui.adsabs.harvard.edu/abs/2001astro.ph..8028P/abstract) - A Practical Theorem on Gravitational Wave Backgrounds \n",
    "* [Enoki+Nagashima-2007](https://ui.adsabs.harvard.edu/abs/2007PThPh.117..241E/abstract) -  The Effect of Orbital Eccentricity on Gravitational Wave Background Radiation from Supermassive Black Hole Binaries\n",
    "* [Sesana+2008](https://ui.adsabs.harvard.edu/abs/2008MNRAS.390..192S/abstract) - The stochastic gravitational-wave background from massive black hole binary systems: implications for observations with Pulsar Timing Arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../notebooks/init.ipy\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Builtin packages\n",
    "from importlib import reload\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# standard secondary packages\n",
    "import astropy as ap\n",
    "import h5py\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "# development packages\n",
    "import kalepy as kale\n",
    "import kalepy.utils\n",
    "import kalepy.plot\n",
    "\n",
    "# --- Holodeck ----\n",
    "import holodeck as holo\n",
    "import holodeck.sam\n",
    "from holodeck import cosmo, utils, plot\n",
    "from holodeck.constants import MSOL, PC, YR, MPC, GYR, SPLC, NWTG\n",
    "import holodeck.gravwaves\n",
    "import holodeck.evolution\n",
    "import holodeck.population\n",
    "\n",
    "# Silence annoying numpy errors\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Plotting settings\n",
    "mpl.rc('font', **{'family': 'serif', 'sans-serif': ['Times'], 'size': 15})\n",
    "mpl.rc('lines', solid_capstyle='round')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "mpl.style.use('default')   # avoid dark backgrounds from dark theme vscode\n",
    "plt.rcParams.update({'grid.alpha': 0.5})\n",
    "\n",
    "# Load log and set logging level\n",
    "log = holo.log\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figax_gwb(**kw):\n",
    "    kwargs = dict(xlabel='GW Frequency $[\\mathrm{yr}^{-1}]$', ylabel='Characteristic Strain')\n",
    "    kwargs.update(kw)\n",
    "    fig, ax = plot.figax(**kwargs)\n",
    "    plot._twin_hz(ax, fs=10)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define LaTeX macros/commands**\n",
    "\n",
    "$\\newcommand{\\mchirp}{\\mathcal{M}}$\n",
    "$\\newcommand{\\msol}{M_\\odot}$\n",
    "\n",
    "$\\newcommand{\\lr}[1]{\\left({#1}\\right)}$\n",
    "$\\newcommand{\\lrangle}[1]{\\langle{#1}\\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idealized GWB Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a population of binaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will randomly create some number of sample binaries, as a toy model.  From that, we will calculate a number-density distribution of binaries.  Typically, the number density would be calculated directly (e.g. from semi-analytic models), but here we start from a finite population to more easily allow for cross-checking the results.\n",
    "\n",
    "The number-density distribution can also be used to construct realizations of discrete binary populations, including populations over the full universe.  Because we are not starting from the number-density, we can also cross-check these discretized populations with our starting population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 1e6                   #: number of starting, sample binaries\n",
    "MASS_EXTR = [1e6, 1e10]     #: range of total-masses to construct (units of [Msol])\n",
    "\n",
    "# Specify PTA frequency range of interest\n",
    "TMAX = (20.0 * YR)          #: maximum observing time in units of [sec]\n",
    "NFREQS = 100                #: number of frequency bins to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct target PTA frequency bins.\n",
    "fobs_gw = np.arange(1, NFREQS+1) / TMAX     #: frequency bin-centers in units of [Hz]\n",
    "df = fobs_gw[0] / 2                         #: half of frequency bin-width\n",
    "fobs_gw_edges = np.concatenate([fobs_gw - df, [fobs_gw[-1] + df]])   #: frequency bin-edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct sample binary population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct binary total-masses\n",
    "MASS_DENS_POWER_LAW = -3           #: power-law index of mass-distribution\n",
    "\n",
    "# Choose random masses following power-law distribution with given index in number-density\n",
    "rr = np.random.random(size=int(NUM))\n",
    "plaw = MASS_DENS_POWER_LAW + 1.0\n",
    "masses = np.array(MASS_EXTR) ** plaw\n",
    "masses = (masses[0] + (masses[1] - masses[0])*rr) ** (1./plaw)\n",
    "masses *= MSOL\n",
    "del rr\n",
    "\n",
    "# Set fixed values of redshift and mass-ratio\n",
    "redz = 0.05      #: redshift of all binaries\n",
    "mrat = 0.3      #: mass-ratio of all binaries\n",
    "\n",
    "# Plot mass distributon\n",
    "fig, ax = plot.figax(xlabel='Total Mass $[M_\\odot]$', ylabel='Number Density $[1/M_\\odot]$')\n",
    "kale.dist1d((masses/MSOL), carpet=False, density=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Number-Density distribution of MBH binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS = 123     #: number of mass-bins for number-density distribution\n",
    "\n",
    "# mbin_edges = zmath.spacing(masses, 'log', NBINS+1)\n",
    "mbin_edges = MSOL * np.logspace(*np.log10(MASS_EXTR), NBINS+1)     #: edges of mass-bins, units of [gram]\n",
    "mbin_cents = 0.5 * (mbin_edges[:-1] + mbin_edges[1:])              #: centers of mass-bins, units of [gram]\n",
    "\n",
    "# Volume of the Universe out to the given redshift\n",
    "vcom = cosmo.comoving_volume(redz).cgs.value    #: Comoving volume in units of [cm^3]\n",
    "\n",
    "# Calculate binary number-density, units of [1/ (cm^3 * g)]\n",
    "ndens, *_ = sp.stats.binned_statistic(masses, None, statistic='count', bins=mbin_edges)   # histogram the binaries\n",
    "ndens /= np.diff(mbin_edges)    #: divide by the bin-widths to get number-density\n",
    "ndens /= vcom                   #: divide by volume to get a comoving volume-density\n",
    "\n",
    "fig, ax = plot.figax(xlabel='Total Mass [$M_\\odot$]', ylabel='Differential number density $[M_\\odot^{-1} \\, \\\\mathrm{Mpc}^{-3}]$')\n",
    "plot.draw_hist_steps(ax, mbin_edges/MSOL, ndens*MSOL*(MPC**3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Analytic (SA) Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GWB characteristic strain spectrum can be calculated **semi-analytically** using a volumetric number-density of sources $n(M, q, z) = dN/dV_c$, as [Phinney 2001, Eq. 5] or [Enoki & Nagashima 2007, Eq. 3.6]:\n",
    "\n",
    "$$ h_c^2 = \\frac{4G}{\\pi c^2 f} \\int dM \\, dq \\, dz \\, \\frac{d^3 n(M, q, z)}{dM \\, dq \\, dz} \\, \\left( \\frac{dE_{GW}(M, q)}{d f_r}\\right)_{f_r = f(1+z)}$$\n",
    "\n",
    "Assuming circular, GW-driven orbits, this can be rewritten as [Enoki & Nagashima 2007, Eq.3.11]:\n",
    "\n",
    "$$ h_c^2 = \\frac{4\\pi}{3 c^2} (\\pi f)^{-4/3} \\int dM \\, dq \\, dz \\, \\frac{d^3 n(M, q, z)}{dM \\, dq \\, dz} \\, \\frac{(G\\mathcal{M})^{5/3}}{(1+z)^{1/3}}$$\n",
    "\n",
    "Typically the number density will be calculated based on observations or phenomenological grounds.  For example, rough estimates of the occurrence rates of binaries, or based on Semi-Analytic or Semi-Empirical Models (SAMs / SEMs) of populations of galaxies, galaxy mergers, and black hole binary populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GWB assuming circular, GW-driven evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bin-edges in chirp-mass (units of [gram])\n",
    "mchirp_edges = utils.chirp_mass_mtmr(mbin_edges, mrat)\n",
    "mchirp_cents = 0.5 * (mchirp_edges[:-1] + mchirp_edges[1:])\n",
    "\n",
    "# Construct the integrand\n",
    "integrand = ndens * np.power(NWTG * mchirp_cents, 5.0/3.0) * np.power(1+redz, -1.0/3.0)\n",
    "\n",
    "# sum over bins\n",
    "gwb_sa = ((4.0 * np.pi) / (3 * SPLC**2)) * np.power(np.pi*fobs_gw, -4.0/3.0) * np.sum(integrand * np.diff(mbin_edges))\n",
    "gwb_sa = np.sqrt(gwb_sa)\n",
    "\n",
    "# plot GWB\n",
    "xx = fobs_gw * YR\n",
    "fig, ax = figax_gwb()\n",
    "ax.plot(xx, gwb_sa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GWB can also be calculated explicitly from the full population of binaries in the universe [Sesana et al. 2008, Eq.~10], \n",
    "$$h_c^2(f) = \\int_0^\\infty \\!\\! dM \\, dq \\, dz \\; \\frac{d^4 N}{dM \\, dq \\, dz \\, d\\ln f_r} \\; h^2(f_r),$$\n",
    "\n",
    "where the spectral GW strain (*not* characteristic strain) for a circular binary is,\n",
    "\n",
    "$$h(f_r) = \\frac{8}{10^{1/2}} \\frac{(G\\mathcal{M})^{5/3}}{c^4 d_c} (2\\pi f_r)^{2/3}.$$\n",
    "\n",
    "From [Sesana et al. 2008, Eq.6] we can write,\n",
    "\n",
    "$$\\frac{d^4 N}{dM \\, dq \\, dz \\, d\\ln f_r} = \\frac{d^3 n_c}{dM \\, dq \\, dz} \\frac{dz}{dt} \\frac{dt}{d\\ln f_r} \\frac{d V_c}{dz}.$$\n",
    "\n",
    "The standard cosmographic relations are [Hogg 1999],\n",
    "\n",
    "$$\\frac{dz}{dt} = H_0 (1+z) E(z) \\\\\n",
    "    \\frac{d V_c}{dz} = 4\\pi \\frac{c}{H_0} \\frac{d_c^2}{E(z)} \\\\\n",
    "    d_L = d_c \\, (1+z)$$\n",
    "\n",
    "Combining these, we obtain:\n",
    "\n",
    "$$h_c^2(f) = \\int_0^\\infty \\!\\! dM \\, dq \\, dz \\; \\frac{d^3 n_c}{dM \\, dq \\, dz} \\, h^2(f_r) \\, 4\\pi c \\, d_c^2 (1+z) \\, \\frac{f_r}{df_r / dt}.$$\n",
    "\n",
    "The hardening timescale for a circular, GW-driven binary is:\n",
    "\n",
    "$$\\tau_{GW} \\equiv \\frac{f_r}{\\left[df_r/dt\\right]_{GW}} = \\frac{5}{96} \\frac{c^5}{(G \\mathcal{M})^{5/3}} (2\\pi f_r)^{-8/3}.$$\n",
    "\n",
    "Plugging this in to the previous relation gives:\n",
    "\n",
    "$$h_c^2(f) = \\frac{20\\pi c^6}{96} \\int_0^\\infty \\!\\! dM \\, dq \\, dz \\; \\frac{d^3 n_c}{dM \\, dq \\, dz} \\, h^2(f_r) \\, \\frac{d_c^2 (1+z)}{(G \\mathcal{M})^{5/3}} (2\\pi f_r)^{-8/3}.$$\n",
    "\n",
    "Note that this is ultimately the same expression as for the Semi-Analytic calculation previously.  But we can use it in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwb_number_from_ndens(ndens, medges, mc_cents, dcom, fro):\n",
    "    \"\"\"Convert from binary (volume-)density [dn/dM], to binary number [dN/dM].\n",
    "    \n",
    "    Effectively, [Sesana+2008] Eq.6.\n",
    "    \n",
    "    \"\"\"\n",
    "    # `fro` = rest-frame orbital frequency\n",
    "    integrand = ((20*np.pi*(SPLC**6))/96) * ndens * np.diff(medges)\n",
    "    integrand *= (dcom**2) * (1.0 + redz) * np.power(NWTG * mc_cents, -5.0/3.0)\n",
    "    integrand = integrand[:, np.newaxis] * np.power(2.0*np.pi*fro, -8.0/3.0)\n",
    "    return integrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from observer-frame GW frequency to rest-frame orbital frequency (assuming circular binaries)\n",
    "frst_orb = fobs_gw[np.newaxis, :] * (1.0 + redz) / 2.0\n",
    "\n",
    "# Get comoving distance, units of [cm]\n",
    "dcom = cosmo.comoving_distance(redz).cgs.value\n",
    "\n",
    "# Calculate spectral strain of binaries at bin-centers\n",
    "hs_mc = (8.0 / np.sqrt(10)) * np.power(NWTG * mchirp_cents, 5.0/3.0) / (dcom * (SPLC**4))\n",
    "hs_mc = hs_mc[:, np.newaxis] * np.power(2*np.pi*frst_orb, 2.0/3.0) \n",
    "\n",
    "# Get the distribution of number of binaries\n",
    "integrand = gwb_number_from_ndens(ndens, mbin_edges, mchirp_cents, dcom, frst_orb)\n",
    "\n",
    "# Sum over bins to get GWB amplitude\n",
    "gwb_mc = np.sum(integrand * (hs_mc**2), axis=0)\n",
    "gwb_mc = np.sqrt(gwb_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figax_gwb()\n",
    "\n",
    "xx = fobs_gw * YR\n",
    "ax.plot(xx, gwb_sa, 'k--', alpha=0.5, label='SA')\n",
    "ax.plot(xx, gwb_mc, label='MC', lw=2.0, alpha=0.7)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"MC\" (Monte-Carlo) and \"SA\" (semi-analytic) calculations should match perfectly so far.  In the following section we will use the \"MC\" expression to actually perform a Monte-Carlo sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization / Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous calculations (both semi-analytic, and the re-written version) assume a smooth continuous distribution of binaries.  Binaries, however, are discrete: the population is composed of individual systems, and there cannot be fractional systems (as is implicitly assumed above).  To correct this, we can discretize our population into integer multiples of binaries.  At the same time, we can also take into account some measure of cosmic variance - in the form of Poisson variations --- which also gives us multiple 'realizations' of the population.  For a given bin of binaries, instead of using the fractional expectation-value number of binaries, we will draw from a Poisson distribution centered around that value.\n",
    "\n",
    "Note that we are still restricting ourselves to the binned population.  i.e. instead of individual binaries across the parameter space, we are still consider the ``N_i`` binaries in each parameter bin ``i``.  But now we are ensuring that ``N_i`` is an integer, and we can also construct multiple realizations of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NREALS = 100    #: choose a number of realizations to model\n",
    "\n",
    "\"\"\"\n",
    "NOTE: `gwb_number_from_ndens` returns ``dN/dln(f)``.  We want to create realizations based on ``N``\n",
    "    the actualy number of binaries.  So we multiply by ``Delta ln(f)``, to get the number of\n",
    "    binaries in each frequency bin (``Delta N_i``).  Then we calculate the discretizations.\n",
    "    Then we divide by ``Delta ln(f)`` again, to get the number of binaries per frequency bin,\n",
    "    needed for the GW characteristic strain calculation.\n",
    "\"\"\"\n",
    "\n",
    "integrand = gwb_number_from_ndens(ndens, mbin_edges, mchirp_cents, dcom, frst_orb)\n",
    "# get the number of binaries in each frequency bin\n",
    "integrand = integrand * np.diff(np.log(fobs_gw_edges))\n",
    "\n",
    "num_exp = np.sum(integrand[:, 0])\n",
    "print(f\"Expected number of binaries in zero freq bin: {num_exp:.4e}\")\n",
    "\n",
    "# Calculate \"realizations\" by Poisson sampling distribution of binary number\n",
    "realized = np.random.poisson(integrand[..., np.newaxis], size=integrand.shape + (NREALS,))\n",
    "\n",
    "# convert back to number of binaries per log-frequency interval, for GWB calculation\n",
    "realized = realized / np.diff(np.log(fobs_gw_edges))[np.newaxis, :, np.newaxis]\n",
    "\n",
    "num_real = np.sum(realized[:, 0, :], axis=0)\n",
    "num_real_ave = np.mean(num_real)\n",
    "num_real_std = np.std(num_real)\n",
    "print(f\"Realized number of binaries in zero freq bin: {num_real_ave:.4e} Â± {num_real_std:.2e}\")\n",
    "\n",
    "# Calculate GWB amplitude\n",
    "gwb_mc_real = np.sum(realized * (hs_mc**2)[..., np.newaxis], axis=0)\n",
    "gwb_mc_real = np.sqrt(gwb_mc_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figax_gwb()\n",
    "xx = fobs_gw * YR\n",
    "ax.plot(xx, gwb_sa, 'k--', alpha=0.5, label='SA')\n",
    "ax.plot(xx, gwb_mc, label='MC', lw=2.0, alpha=0.7)\n",
    "\n",
    "color = 'r'\n",
    "gwb_mc_med = np.median(gwb_mc_real, axis=-1)\n",
    "gwb_mc_span = np.percentile(gwb_mc_real, [25, 75], axis=-1)\n",
    "ax.plot(xx, gwb_mc_med, lw=0.5, color=color)\n",
    "ax.fill_between(xx, *gwb_mc_span, alpha=0.25, color=color, label='MC realized')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Population Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number density was calculated from a finite number of binaries, in a finite volume.  Instead of going through the number-density as an intermediate quantity (i.e. binning sample binaries), just use the finite number of binaries directly to calculate the GWB.\n",
    "\n",
    "$$\n",
    "    \\frac{d^3 n_c}{dM \\, dq \\, dz} \\, dM \\, dq \\, dz\n",
    "        \\rightarrow \\frac{1}{V_c} \\sum_i  \\delta(M < M_i < M + \\Delta M) \\cdot \\delta(q < q_i < q + \\Delta q) \\cdot \\delta(z < z_i < z + \\Delta z) \\, F(M, q, z) \\\\\n",
    "        \\rightarrow \\frac{1}{V_c} \\sum_i F(M_i \\,,\\, q_i \\,,\\, z_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcom = cosmo.comoving_distance(redz).cgs.value\n",
    "frst_orb = fobs_gw[np.newaxis, :] * (1.0 + redz) / 2.0\n",
    "mchirp = utils.chirp_mass_mtmr(masses, mrat)\n",
    "\n",
    "hs_fin = (8.0 / np.sqrt(10)) * np.power(NWTG * mchirp, 5.0/3.0) / (dcom * (SPLC**4))\n",
    "hs_fin = hs_fin[:, np.newaxis] * np.power(2*np.pi*frst_orb, 2.0/3.0) \n",
    "\n",
    "integrand = ((20*np.pi*(SPLC**6))/96) / vcom\n",
    "integrand *= (dcom**2) * (1.0 + redz) * np.power(NWTG * mchirp, -5.0/3.0)\n",
    "integrand = integrand[:, np.newaxis] * np.power(2.0*np.pi*frst_orb, -8.0/3.0)\n",
    "\n",
    "gwb_fin = np.sum(integrand * (hs_fin**2), axis=0)\n",
    "gwb_fin = np.sqrt(gwb_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figax_gwb()\n",
    "xx = fobs_gw * YR\n",
    "ax.plot(xx, gwb_sa, 'k--', label='SA', alpha=0.5, lw=2.0)\n",
    "ax.plot(xx, gwb_mc, lw=2.0, alpha=0.7, label='MC')\n",
    "ax.plot(xx, gwb_fin, lw=2.0, alpha=0.75, label='Finite')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization / Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way to the MC calculation above, we can discretize and calculate multiple realizations from the starting finite population.  This is a little strange: we start with a finite population, and then construct multiple, new discrete populations from this.  There is a key difference from the starting populations and the new ones: the starting population represents only a fixed volume, while the realizations are very explicitly full Universes.  In this example, the difference is trivial, but if the starting population comes from a finite volume (for example a cosmological hydrodynamic simulation), then the difference is much more important (and useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this is quite slow, so only construct a single realization!\n",
    "DISCRETIZE_FINITE_FLAG = True\n",
    "\n",
    "if DISCRETIZE_FINITE_FLAG:\n",
    "\n",
    "    _dlnf = np.diff(np.log(fobs_gw_edges))\n",
    "    real = np.random.poisson(integrand * _dlnf) / _dlnf\n",
    "    gwb_fin_real = np.sum(real * (hs_fin**2), axis=0)\n",
    "    gwb_fin_real = np.sqrt(gwb_fin_real)\n",
    "\n",
    "    fig, ax = figax_gwb()\n",
    "    xx = fobs_gw * YR\n",
    "    ax.plot(xx, gwb_sa, 'k--', alpha=0.5, label='SA')\n",
    "    ax.plot(xx, gwb_fin, lw=3.0, alpha=0.6, ls=':', label='Finite')\n",
    "    col, = ax.plot(xx, gwb_mc_med, lw=2.0, alpha=0.7, label='MC realized')\n",
    "    col = col.get_color()\n",
    "    ax.fill_between(xx, *gwb_mc_span, alpha=0.25, color=col, label='MC realized')\n",
    "\n",
    "    color = 'r'\n",
    "    ax.plot(xx, gwb_fin_real, lw=0.75, color=color, alpha=0.5, label='Finite realized')\n",
    "\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN DEVELOPMENT / TESTING ::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Re)Sampling Binned Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MC method, we 'discretized' the population to account for finite number effects and Poisson sampling (i.e. a model for cosmic variance).  This was still done to the distribution of binaries in a bin-wise fashion: we would get a 'discretized' number of binaries per bin, for each realization.\n",
    "\n",
    "Here we will construct populations that attempt to better Monte-Carlo sample the binary parameter space.  This is done using the `kalepy.sample_outliers` function.  The idea is that the most interesting parts of parameter space are the bins with expectation values for order-unity binaries, i.e.$\\lrangle{N_i} \\sim 1$.  The motivation is that bins with $\\lrangle{N_i} \\ll 1$ are unlikely to ever produce binaries, and bins with $\\lrangle{N_i} \\gg 1$ are accurately represented by the bin-centroid value, instead of sampling individual binaries explicitly.  The `kalepy.sample_outliers` function thus returns weighted bin-centroids above some critical expectation value, and samples individual binaries for bins below that critical value.\n",
    "\n",
    "The function call looks like, `kalepy.sample_outliers(edges, density, threshold, mass=None)`.  The arguments are as follows:\n",
    "* `edges` : the grid-edges of the parameter space being sampled.  This is a list of arrays, with one array for each dimension of the space.\n",
    "* `density` : the number-density, evaluated at grid-edges, that is sampled from.  \n",
    "* `threshold` : the number of binaries per-bin, below which each binary is sampled.  Bins above this value will return a weighted centroid.  Bins below this value will return the appropriate number of individual binaries each with weight equal to unity.\n",
    "* `mass` : the number of binaries that should be sampled in each bin.  This is an optional argument, and if it is not provided, then `density` is integrated over to calculate `mass`.\n",
    "\n",
    "The return values are `vals, weights` which are:\n",
    "* `vals` : the samples binary parameters, shaped `(D, S)` for `D` dimensions of parameter space, and `S` total number of samples - including both individual binaries, and grid centroids.\n",
    "* `weights` : the weight of each sample, either equal to unity for individual binaries (i.e. for bins below the sampling threshold), or equal to the bin-mass (for bins above the sampling threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: sampling the population is somewhat delicate and must be done with care!  See additional notes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrap_cents_to_edges(grid):\n",
    "    ndim = grid.ndim\n",
    "    vals = np.copy(grid)\n",
    "\n",
    "    # Extrapolate to one more point each left and right, along each axis progressively\n",
    "    # (A,B,C,...) ==> (A+2,B,C,...) ==> (A+2,B+2,C,...) ==>  ...  ==> (A+2, B+2, C+2, ...)\n",
    "    for ax in range(ndim):\n",
    "        vals = np.moveaxis(vals, ax, 0)\n",
    "        ll = 2*vals[0] - vals[1]\n",
    "        rr = 2*vals[-1] - vals[-2]\n",
    "        vals = np.concatenate([[ll], vals, [rr]], axis=0)\n",
    "        vals = np.moveaxis(vals, 0, ax)\n",
    "\n",
    "    # Interpolate to mid-points along each axis\n",
    "    # (A+2,B+2,C+2,...) ==> (A+1,B+2,C+2,...) ==> (A+1,B+1,C,...) ==>  ...  ==> (A+1, B+1, C+1, ...)\n",
    "    for ax in range(ndim):\n",
    "        vals = np.moveaxis(vals, ax, 0)\n",
    "        vals = 0.5 * (vals[:-1] + vals[1:])\n",
    "        vals = np.moveaxis(vals, 0, ax)\n",
    "\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = frst_orb[0, 0]\n",
    "frst_orb_edges = np.concatenate([frst_orb[0] - _df/2, [frst_orb[0][-1] + _df/2]])\n",
    "\n",
    "number_resamp = gwb_number_from_ndens(ndens, mbin_edges, mchirp_cents, dcom, frst_orb)\n",
    "number_resamp *= np.diff(np.log(fobs_gw_edges))\n",
    "print(f\"{number_resamp.sum()=:.4e}\")\n",
    "\n",
    "# Convert to differential-density number\n",
    "sample_ndens = number_resamp / np.diff(mbin_edges)[:, np.newaxis]\n",
    "sample_ndens = sample_ndens / np.diff(frst_orb_edges)[np.newaxis, :]\n",
    "sample_ndens = extrap_cents_to_edges(sample_ndens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Full Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses the `kalepy.sample_grid` method to sample *ALL* binaries.  This is extremely slow, and memory intensive.  For large-enough number of binaries, it will likely crash.  Use with caution, and only when the total number of binaries being samples is within the capabilities of the computer -- typically $\\lrangle{N_\\mathrm{total}} \\lesssim 10^7$ or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FULL_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NREALS = 10\n",
    "\n",
    "#!!! NOTE: this is *prohibitively* slow !!!#\n",
    "\n",
    "if SAMPLE_FULL_FLAG:\n",
    "    log.warning(\"!SAMPLING FULL POPULATION IS REALLY SLOW AND MEMORY INTENSIVE!\")\n",
    "    gwb_resamp_full = np.zeros((fobs_gw.size, NREALS))\n",
    "\n",
    "    for ii in utils.tqdm(range(NREALS)):\n",
    "        sample_edges = [np.log10(mbin_edges), np.log(frst_orb_edges)]\n",
    "        vals = kale.sample_grid(sample_edges, sample_ndens, mass=number_resamp)\n",
    "        mm = 10.0 ** vals[0]\n",
    "        frorb = np.e ** vals[1]\n",
    "\n",
    "        dcom = cosmo.comoving_distance(redz).cgs.value\n",
    "        mchirp = utils.chirp_mass_mtmr(mm, mrat)\n",
    "\n",
    "        hs = (8.0 / np.sqrt(10)) * np.power(NWTG * mchirp, 5.0/3.0) / (dcom * (SPLC**4))\n",
    "        hs = hs * np.power(2*np.pi*frorb, 2.0/3.0) \n",
    "\n",
    "        sepa_isco = 6 * NWTG * mm / SPLC**2\n",
    "        frst_orb_isco = utils.kepler_freq_from_sepa(mm, sepa_isco)\n",
    "        bads = frorb > frst_orb_isco\n",
    "        merged = np.ones_like(bads, dtype=float)\n",
    "        merged[bads] = 0.0\n",
    "\n",
    "        fogw = frorb * 2.0 / (1.0 + redz)\n",
    "        hs = merged * (hs**2)\n",
    "        gwb_resamp_full[:, ii], *_ = sp.stats.binned_statistic(fogw, hs, statistic='sum', bins=fobs_gw_edges)\n",
    "        gwb_resamp_full[:, ii] = np.sqrt(gwb_resamp_full[:, ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_FULL_FLAG:\n",
    "\n",
    "    fig, ax = plot.figax()\n",
    "    xx = fobs_gw * YR\n",
    "    ax.plot(xx, gwb_sa, 'k--', lw=1.5, alpha=0.5)\n",
    "    ax.plot(xx, gwb_fin, 'k:', lw=2.0, alpha=0.5)\n",
    "    gwb_mc_med = np.median(gwb_mc, axis=-1)\n",
    "    ax.plot(xx, gwb_mc_med, color='b', lw=2.0, alpha=0.5)\n",
    "\n",
    "    med = np.median(gwb_resamp_full, axis=-1)\n",
    "    ax.plot(xx, med, lw=0.5, color='r')\n",
    "    ax.fill_between(xx, *np.percentile(gwb_resamp_full, [25, 75], axis=-1), alpha=0.5, color='r')\n",
    "\n",
    "    tw = ax.twinx()\n",
    "    tw.plot(xx, gwb_sa/med, 'r--', alpha=0.5)\n",
    "    tw.plot(xx, gwb_mc_med/med, 'b--', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 'Outliers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier sampling must be performed carefully, as it can be delicate.  Keep in mind some of the following considerations:\n",
    "* bin centroids are used for bins above the threshold.  The number of bins in the distribution must be sufficiently high such that the centroids are good approximations for the true distribution of values.\n",
    "* consider whether to sample in linear or log space for different parameters.  For example, **for the GWB calculation, sampling mass in linear-space produces better results** and it produces centroids nearer to the strain-weighted mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NREALS = 100\n",
    "\n",
    "gwb_resamp_out = np.zeros((fobs_gw.size, NREALS))\n",
    "sample_threshold = 1e2\n",
    "\n",
    "DOWN = 10.0\n",
    "\n",
    "temp = np.copy(number_resamp) / DOWN\n",
    "\n",
    "for ii in utils.tqdm(range(NREALS)):\n",
    "    # sample_edges = [np.log10(mbin_edges), np.log(frst_orb_edges)]\n",
    "    # vals, weights = kale.sample_outliers(sample_edges, sample_ndens, sample_threshold, mass=number_resamp) \n",
    "    # mm = 10.0 ** vals[0]\n",
    "    # frorb = np.e ** vals[1]\n",
    "    \n",
    "    sample_edges = [mbin_edges, frst_orb_edges]\n",
    "    vals, weights = kale.sample_outliers(sample_edges, sample_ndens, sample_threshold, mass=temp)\n",
    "    mm = vals[0]\n",
    "    frorb = vals[1]\n",
    "    weights = weights * DOWN\n",
    "\n",
    "    dcom = cosmo.comoving_distance(redz).cgs.value\n",
    "    mchirp = utils.chirp_mass_mtmr(mm, mrat)\n",
    "\n",
    "    hs = (8.0 / np.sqrt(10)) * np.power(NWTG * mchirp, 5.0/3.0) / (dcom * (SPLC**4))\n",
    "    hs = hs * np.power(2*np.pi*frorb, 2.0/3.0) \n",
    "\n",
    "    fogw = frorb * 2.0 / (1.0 + redz)\n",
    "\n",
    "    hs_1 = (hs**2) * weights\n",
    "    gwb_resamp_out[:, ii], *_ = sp.stats.binned_statistic(fogw, hs_1, statistic='sum', bins=fobs_gw_edges)\n",
    "    gwb_resamp_out[:, ii] = gwb_resamp_out[:, ii] / np.diff(np.log(fobs_gw_edges))\n",
    "\n",
    "gwb_resamp_out = np.sqrt(gwb_resamp_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.figax()\n",
    "xx = fobs_gw * YR\n",
    "ax.plot(xx, gwb_sa, 'k--', lw=1.5, alpha=0.5, label='SA')\n",
    "ax.plot(xx, gwb_fin, 'k:', lw=2.0, alpha=0.5, label='Finite')\n",
    "\n",
    "color = 'b'\n",
    "ax.plot(xx, gwb_mc_med, lw=0.5, color=color)\n",
    "ax.fill_between(xx, *gwb_mc_span, alpha=0.25, color=color, label='MC realized')\n",
    "\n",
    "color = 'r'\n",
    "gwb_resamp_med = np.median(gwb_resamp_out, axis=-1)\n",
    "gwb_resamp_span = np.percentile(gwb_resamp_out, [25, 75], axis=-1)\n",
    "ax.plot(xx, gwb_resamp_med, lw=0.5, color=color, alpha=0.5)\n",
    "ax.fill_between(xx, *gwb_resamp_span, alpha=0.25, color=color, label='Resample')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.figax(yscale='linear', ylabel='Ratio (Resamp/MC)')\n",
    "\n",
    "cc, = ax.plot(xx, gwb_resamp_med/gwb_mc_med, alpha=0.5, label='Median')\n",
    "ax.scatter(xx, gwb_resamp_med/gwb_mc_med, alpha=0.25, color=cc.get_color(), marker='.', s=50)\n",
    "ax.plot(xx, (gwb_resamp_span/gwb_mc_span).T, alpha=0.5, label='Interquartile Boundaries')\n",
    "\n",
    "ax.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "61b4062b24dfb1010f420dad5aa3bd73a4d2af47d0ec44eafec465a35a9d7239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
